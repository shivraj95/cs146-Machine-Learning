{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = np.linspace(0, 1, 101)\n",
    "x = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def likelihood(theta, x, n):\n",
    "    l = np.ones([len(theta), 1])\n",
    "    for i in range(0,len(theta)):\n",
    "        for j in range(0,n):\n",
    "                l[i] *= theta[i]**x[j]*(1-theta[i])**(1-x[j]) \n",
    "    return l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_theta = likelihood(theta, x, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(theta, l_theta)\n",
    "#plt.annotate('max', xy=(theta[l_theta.argmax()], l_theta[60]), xytext=(0.4, .0010),\n",
    "#            arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "#            )\n",
    "plt.axvline(x=theta[l_theta.argmax()])\n",
    "plt.xlabel('$\\Theta$')\n",
    "plt.ylabel('$L(\\Theta)$')\n",
    "plt.savefig('p1_1b.jpg')\n",
    "#plt.show()\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_d_1 = [1, 1, 1, 0,0]\n",
    "x_d_2 = 60*[1] + 40*[0]\n",
    "x_d_3 = 5*[1] + 5*[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_theta_1 = likelihood(theta, x_d_1, 5)\n",
    "l_theta_2 = likelihood(theta, x_d_2, 100)\n",
    "l_theta_3 = likelihood(theta, x_d_3, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.plot(theta, l_theta_1)\n",
    "plt.xlabel('$\\Theta$')\n",
    "plt.ylabel('$L(\\Theta)$')\n",
    "plt.title('n = 5')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(theta, l_theta_2)      \n",
    "plt.xlabel('$\\Theta$')\n",
    "plt.ylabel('$L(\\Theta)$')\n",
    "plt.title('n = 100')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(theta, l_theta_3)\n",
    "plt.xlabel('$\\Theta$')\n",
    "plt.ylabel('$L(\\Theta)$')\n",
    "plt.title('n = 10')\n",
    "plt.savefig('p1_1d.jpg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use only the provided packages!\n",
    "import math\n",
    "import csv\n",
    "from util import *\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# classes\n",
    "######################################################################\n",
    "\n",
    "class Classifier(object) :\n",
    "    \"\"\"\n",
    "    Classifier interface.\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class MajorityVoteClassifier(Classifier) :\n",
    "    \n",
    "    def __init__(self) :\n",
    "        \"\"\"\n",
    "        A classifier that always predicts the majority class.\n",
    "        \n",
    "        Attributes\n",
    "        --------------------\n",
    "            prediction_ -- majority class\n",
    "        \"\"\"\n",
    "        self.prediction_ = None\n",
    "    \n",
    "    def fit(self, X, y) :\n",
    "        \"\"\"\n",
    "        Build a majority vote classifier from the training set (X, y).\n",
    "        \n",
    "        Parameters\n",
    "        --------------------\n",
    "            X    -- numpy array of shape (n,d), samples\n",
    "            y    -- numpy array of shape (n,), target classes\n",
    "        \n",
    "        Returns\n",
    "        --------------------\n",
    "            self -- an instance of self\n",
    "        \"\"\"\n",
    "        majority_val = Counter(y).most_common(1)[0][0]\n",
    "        self.prediction_ = majority_val\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X) :\n",
    "        \"\"\"\n",
    "        Predict class values.\n",
    "        \n",
    "        Parameters\n",
    "        --------------------\n",
    "            X    -- numpy array of shape (n,d), samples\n",
    "        \n",
    "        Returns\n",
    "        --------------------\n",
    "            y    -- numpy array of shape (n,), predicted classes\n",
    "        \"\"\"\n",
    "        if self.prediction_ is None :\n",
    "            raise Exception(\"Classifier not initialized. Perform a fit first.\")\n",
    "        \n",
    "        n,d = X.shape\n",
    "        y = [self.prediction_] * n \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomClassifier(Classifier) :\n",
    "    \n",
    "    def __init__(self) :\n",
    "        \"\"\"\n",
    "        A classifier that predicts according to the distribution of the classes.\n",
    "        \n",
    "        Attributes\n",
    "        --------------------\n",
    "            probabilities_ -- class distribution dict (key = class, val = probability of class)\n",
    "        \"\"\"\n",
    "        self.probabilities_ = None\n",
    "    \n",
    "    def fit(self, X, y) :\n",
    "        \"\"\"\n",
    "        Build a random classifier from the training set (X, y).\n",
    "        \n",
    "        Parameters\n",
    "        --------------------\n",
    "            X    -- numpy array of shape (n,d), samples\n",
    "            y    -- numpy array of shape (n,), target classes\n",
    "        \n",
    "        Returns\n",
    "        --------------------\n",
    "            self -- an instance of self\n",
    "        \"\"\"\n",
    "        \n",
    "        ### ========== TODO : START ========== ###\n",
    "        # part b: set self.probabilities_ according to the training set\n",
    "        \n",
    "        unique, counts = np.unique(y, return_counts=True)        \n",
    "        self.probabilities_ = dict(zip(unique, counts/len(y)))\n",
    "        \n",
    "        ### ========== TODO : END ========== ###\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, seed=1234) :\n",
    "        \"\"\"\n",
    "        Predict class values.\n",
    "        \n",
    "        Parameters\n",
    "        --------------------\n",
    "            X    -- numpy array of shape (n,d), samples\n",
    "            seed -- integer, random seed\n",
    "        \n",
    "        Returns\n",
    "        --------------------\n",
    "            y    -- numpy array of shape (n,), predicted classes\n",
    "        \"\"\"\n",
    "        if self.probabilities_ is None :\n",
    "            raise Exception(\"Classifier not initialized. Perform a fit first.\")\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        ### ========== TODO : START ========== ###\n",
    "        # part b: predict the class for each test example\n",
    "        # hint: use np.random.choice (be careful of the parameters)\n",
    "        \n",
    "        y = np.random.choice([0, 1], size=X.shape[0],replace=True ,p=[self.probabilities_[0],self.probabilities_[1]])\n",
    "        \n",
    "        ### ========== TODO : END ========== ###\n",
    "        \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# functions\n",
    "######################################################################\n",
    "def plot_histograms(X, y, Xnames, yname) :\n",
    "    n,d = X.shape  # n = number of examples, d =  number of features\n",
    "    fig = plt.figure(figsize=(20,15))\n",
    "    nrow = 3; ncol = 3\n",
    "    for i in range(0, d) :\n",
    "        fig.add_subplot (3,3,i+1)  \n",
    "        data, bins, align, labels = plot_histogram(X[:,i], y, Xname=Xnames[i], yname=yname, show = False)\n",
    "        n, bins, patches = plt.hist(data, bins=bins, align=align, alpha=0.5, label=labels)\n",
    "        plt.xlabel(Xnames[i])\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend() #plt.legend(loc='upper left')\n",
    " \n",
    "    plt.savefig('histograms.jpg')\n",
    "\n",
    "\n",
    "def plot_histogram(X, y, Xname, yname, show = True) :\n",
    "    \"\"\"\n",
    "    Plots histogram of values in X grouped by y.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        X     -- numpy array of shape (n,d), feature values\n",
    "        y     -- numpy array of shape (n,), target classes\n",
    "        Xname -- string, name of feature\n",
    "        yname -- string, name of target\n",
    "    \"\"\"\n",
    "    \n",
    "    # set up data for plotting\n",
    "    targets = sorted(set(y))\n",
    "    data = []; labels = []\n",
    "    for target in targets :\n",
    "        features = [X[i] for i in range(0, len(y)) if y[i] == target]\n",
    "        data.append(features)\n",
    "        labels.append('%s = %s' % (yname, target))\n",
    "    \n",
    "    # set up histogram bins\n",
    "    features = set(X)\n",
    "    nfeatures = len(features)\n",
    "    test_range = range(int(math.floor(min(features))), int(math.ceil(max(features)))+1)\n",
    "    if nfeatures < 10 and sorted(features) == test_range:\n",
    "        bins = test_range + [test_range[-1] + 1] # add last bin\n",
    "        align = 'left'\n",
    "    else :\n",
    "        bins = 10\n",
    "        align = 'mid'\n",
    "    \n",
    "    # plot\n",
    "    if show == True:\n",
    "        plt.figure()\n",
    "        n, bins, patches = plt.hist(data, bins=bins, align=align, alpha=0.5, label=labels)\n",
    "        plt.xlabel(Xname)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend() #plt.legend(loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "    return data, bins, align, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(clf, X, y, ntrials=100, test_size=0.2) :\n",
    "    \"\"\"\n",
    "    Computes the classifier error over a random split of the data,\n",
    "    averaged over ntrials runs.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        clf         -- classifier\n",
    "        X           -- numpy array of shape (n,d), features values\n",
    "        y           -- numpy array of shape (n,), target classes\n",
    "        ntrials     -- integer, number of trials\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        train_error -- float, training error\n",
    "        test_error  -- float, test error\n",
    "    \"\"\"\n",
    "    \n",
    "    ### ========== TODO : START ========== ###\n",
    "    # compute cross-validation error over ntrials\n",
    "    # hint: use train_test_split (be careful of the parameters)\n",
    "    train_error = 0.0\n",
    "    test_error = 0.0\n",
    "    for i in range(0, 100):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_train= clf.predict(X_train)\n",
    "        train_error += 1 - metrics.accuracy_score(y_pred_train, y_train, normalize=True)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        test_error += 1 - metrics.accuracy_score(y_pred_test, y_test, normalize=True)\n",
    "    \n",
    "    train_error = train_error/100\n",
    "    test_error = test_error/100\n",
    "    \n",
    "    ### ========== TODO : END ========== ###\n",
    "    \n",
    "    return train_error, test_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_predictions(y_pred, filename, yname=None) :\n",
    "    \"\"\"Write out predictions to csv file.\"\"\"\n",
    "    out = open(filename, 'wb')\n",
    "    f = csv.writer(out)\n",
    "    if yname :\n",
    "        f.writerow([yname])\n",
    "    f.writerows(zip(y_pred))\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# main\n",
    "######################################################################\n",
    "\n",
    "def main():\n",
    "    # load Titanic dataset\n",
    "    titanic = load_data(\"titanic_train.csv\", header=1, predict_col=0)\n",
    "    X = titanic.X; Xnames = titanic.Xnames\n",
    "    y = titanic.y; yname = titanic.yname\n",
    "    n,d = X.shape  # n = number of examples, d =  number of features\n",
    "    \n",
    "    \n",
    "    \n",
    "    #========================================\n",
    "    #part a: plot histograms of each feature\n",
    "    #print 'Plotting...'\n",
    "    #for i in range(0, d) :\n",
    "    plot_histograms(X, y, Xnames, yname=yname)\n",
    "    \n",
    "       \n",
    "    #========================================\n",
    "    # train Majority Vote classifier on data\n",
    "    #print 'Classifying using Majority Vote...'\n",
    "    #clf = MajorityVoteClassifier() # create MajorityVote classifier, which includes all model parameters\n",
    "    #clf.fit(X, y)                  # fit training data using the classifier\n",
    "    #y_pred = clf.predict(X)        # take the classifier and run it on the training data\n",
    "    #train_error = 1 - metrics.accuracy_score(y, y_pred, normalize=True)\n",
    "    #print('\\t-- training error: %.3f' % train_error)\n",
    "    \n",
    "    \n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part b: evaluate training error of Random classifier\n",
    "    #print 'Classifying using Random...'\n",
    "    #clf_rand = RandomClassifier() # create MajorityVote classifier, which includes all model parameters\n",
    "    #clf_rand.fit(X, y)                  # fit training data using the classifier\n",
    "    #y_pred_rand = clf_rand.predict(X)        # take the classifier and run it on the training data\n",
    "    #train_error = 1 - metrics.accuracy_score(y, y_pred_rand, normalize=True)\n",
    "    #print('\\t-- training error: %.3f' % train_error)\n",
    "    \n",
    "    \n",
    "    ### ========== TODO : END ========== ###\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part c: evaluate training error of Decision Tree classifier\n",
    "    # use criterion of \"entropy\" for Information gain \n",
    "    #print 'Classifying using Decision Tree...'\n",
    "    #clf = DecisionTreeClassifier(criterion='entropy',random_state=0)\n",
    "    #clf.fit(X, y)    \n",
    "    #y_pred = clf.predict(X) \n",
    "    #train_error = 1 - metrics.accuracy_score(y, y_pred, normalize=True)\n",
    "    #print('\\t-- training error: %.3f' % train_error)\n",
    "    ### ========== TODO : END ========== ###\n",
    "    \n",
    "    \n",
    "    \n",
    "    # note: uncomment out the following lines to output the Decision Tree graph\n",
    "    \"\"\"\n",
    "    # save the classifier -- requires GraphViz and pydot\n",
    "    import StringIO, pydot\n",
    "    from sklearn import tree\n",
    "    dot_data = StringIO.StringIO()\n",
    "    tree.export_graphviz(clf, out_file=dot_data,\n",
    "                         feature_names=Xnames)\n",
    "    graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "    graph.write_pdf(\"dtree.pdf\") \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part d: use cross-validation to compute average training and test error of classifiers\n",
    "    \"\"\"\n",
    "    clf_maj = MajorityVoteClassifier()\n",
    "    clf_rand = RandomClassifier()\n",
    "    #clf_dec = DecisionTreeClassifier(criterion='entropy')\n",
    "    train_error_maj, test_error_maj = error(clf_maj, X, y)\n",
    "    train_error_rand, test_error_rand = error(clf_rand, X, y)\n",
    "    #train_error_dec, test_error_dec = error(clf_dec, X, y)\n",
    "    \n",
    "    #print('Investigating various classifiers...')\n",
    "    #print('Majority Vote Classifier Train Error: ', train_error_maj, '\\nMajority Vote Classifier Test Error: ', test_error_maj)\n",
    "    #print('\\nRandom Classifier Train Error: ', train_error_rand, '\\nRandom Classifier Test Error: ', test_error_rand)\n",
    "    #print('\\nDecision Tree Classifier Train Error: ', train_error_dec, '\\nDecision Tree Classifier Test Error: ', test_error_dec)\n",
    "\n",
    "\n",
    "    ### ========== TODO : END ========== ###\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part e: investigate decision tree classifier with various depths\n",
    "    \n",
    "    print('Investigating depths...')\n",
    "    \n",
    "    train_error = [0]*20\n",
    "    test_error = [0]*20\n",
    "    depth = range(1, 21)\n",
    "    for i in range(1, 21):\n",
    "        clf_dec = DecisionTreeClassifier(criterion='entropy', max_depth = i)\n",
    "        train_error[i-1], test_error[i-1] = error(clf_dec, X, y)\n",
    "    \n",
    "    test_error_maj_depth = [test_error_maj]*20\n",
    "    test_error_rand_depth = [test_error_rand]*20\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(depth,train_error, 'r',label='Decision Tree Training Error')\n",
    "    plt.plot(depth,test_error, '-',label='Decision Tree Testing Error')\n",
    "    plt.plot(depth, test_error_maj_depth, 'b',label='Majority Vote Classifier Test Error')\n",
    "    plt.plot(depth, test_error_rand_depth, 'g', label='Random Classsifier Test Error')\n",
    "    plt.xlabel('Depth Level')\n",
    "    plt.ylabel('Error')\n",
    "    plt.legend(loc=2)\n",
    "    x1,x2,y1,y2 = plt.axis()\n",
    "    plt.axis((x1,x2,0,1.0))\n",
    "\n",
    "    plt.savefig('p1_4e.jpg')\n",
    "    plt.close()\n",
    "    #plt.show()\n",
    "    \n",
    "    \n",
    "    ### ========== TODO : END ========== ###\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part f: investigate decision tree classifier with various training set sizes\n",
    "    #print 'Investigating training set sizes...'\n",
    "    \"\"\"\n",
    "    train_error_dec = [0]*19\n",
    "    test_error_dec = [0]*19\n",
    "    test_error_maj = [0]*19\n",
    "    test_error_rand = [0]*19\n",
    "    j = 0\n",
    "    for i in range(5, 100, 5):\n",
    "        size = (1-i/100)\n",
    "        clf_maj = MajorityVoteClassifier()\n",
    "        clf_rand = RandomClassifier()\n",
    "        clf_dec = DecisionTreeClassifier(criterion='entropy', max_depth = 3)\n",
    "        train_error_maj, test_error_maj[j] = error(clf_maj, X, y, test_size=size)\n",
    "        train_error_rand, test_error_rand[j] = error(clf_rand, X, y, test_size = size)\n",
    "        train_error_dec[j], test_error_dec[j] = error(clf_dec, X, y, test_size = size)\n",
    "        j +=1\n",
    "    \n",
    "    x_axis = np.linspace(.05, .95, 19)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(x_axis, train_error_dec, 'r',label='Decision Tree Training Error')\n",
    "    plt.plot(x_axis, test_error_dec, '-',label='Decision Tree Testing Error')\n",
    "    plt.plot(x_axis, test_error_maj, 'b',label='Majority Vote Classifier Test Error')\n",
    "    plt.plot(x_axis, test_error_rand, 'g', label='Random Classsifier Test Error')\n",
    "    plt.xlabel('Training Size')\n",
    "    plt.ylabel('Error')\n",
    "    plt.legend(loc=2)\n",
    "    x1,x2,y1,y2 = plt.axis()\n",
    "    plt.axis((x1,x2,0,1.0))\n",
    "    plt.savefig('p1_4f.jpg')\n",
    "    plt.close()\n",
    "                     \n",
    "                      \n",
    "    ### ========== TODO : END ========== ###\n",
    "    \n",
    "       \n",
    "    #print 'Done'\n",
    "    \"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
