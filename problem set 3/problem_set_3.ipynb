{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# !!! MAKE SURE TO USE SVC.decision_function(X), NOT SVC.predict(X) !!!\n",
    "# (this makes ``continuous-valued'' predictions)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# functions -- input/output\n",
    "######################################################################\n",
    "\n",
    "def read_vector_file(fname):\n",
    "    \"\"\"\n",
    "    Reads and returns a vector from a file.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        fname  -- string, filename\n",
    "        \n",
    "    Returns\n",
    "    --------------------\n",
    "        labels -- numpy array of shape (n,)\n",
    "                    n is the number of non-blank lines in the text file\n",
    "    \"\"\"\n",
    "    return np.genfromtxt(fname)\n",
    "\n",
    "\n",
    "def write_label_answer(vec, outfile):\n",
    "    \"\"\"\n",
    "    Writes your label vector to the given file.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        vec     -- numpy array of shape (n,) or (n,1), predicted scores\n",
    "        outfile -- string, output filename\n",
    "    \"\"\"\n",
    "    \n",
    "    # for this project, you should predict 70 labels\n",
    "    if(vec.shape[0] != 70):\n",
    "        print(\"Error - output vector should have 70 rows.\")\n",
    "        print(\"Aborting write.\")\n",
    "        return\n",
    "    \n",
    "    np.savetxt(outfile, vec)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# functions -- feature extraction\n",
    "######################################################################\n",
    "\n",
    "def extract_words(input_string):\n",
    "    \"\"\"\n",
    "    Processes the input_string, separating it into \"words\" based on the presence\n",
    "    of spaces, and separating punctuation marks into their own words.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        input_string -- string of characters\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        words        -- list of lowercase \"words\"\n",
    "    \"\"\"\n",
    "    \n",
    "    for c in punctuation :\n",
    "        input_string = input_string.replace(c, ' ' + c + ' ')\n",
    "    return input_string.lower().split()\n",
    "\n",
    "\n",
    "def extract_dictionary(infile):\n",
    "    \"\"\"\n",
    "    Given a filename, reads the text file and builds a dictionary of unique\n",
    "    words/punctuations.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        infile    -- string, filename\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        word_list -- dictionary, (key, value) pairs are (word, index)\n",
    "    \"\"\"\n",
    "    \n",
    "    word_list = {}\n",
    "    with open(infile, 'r') as fid:\n",
    "        ### ========== TODO : START ========== ###\n",
    "        # part 1a: process each line to populate word_list\n",
    "        i = 0\n",
    "        for line in fid:\n",
    "            new_words = extract_words(line)\n",
    "            for words in new_words:\n",
    "                if words in word_list:\n",
    "                    continue\n",
    "                else:\n",
    "                    word_list[words] = i\n",
    "                    i += 1\n",
    "        ### ========== TODO : END ========== ###\n",
    "\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_feature_vectors(infile, word_list):\n",
    "    \"\"\"\n",
    "    Produces a bag-of-words representation of a text file specified by the\n",
    "    filename infile based on the dictionary word_list.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        infile         -- string, filename\n",
    "        word_list      -- dictionary, (key, value) pairs are (word, index)\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        feature_matrix -- numpy array of shape (n,d)\n",
    "                          boolean (0,1) array indicating word presence in a string\n",
    "                            n is the number of non-blank lines in the text file\n",
    "                            d is the number of unique words in the text file\n",
    "    \"\"\"\n",
    "    \n",
    "    num_lines = sum(1 for line in open(infile,'U'))\n",
    "    num_words = len(word_list)\n",
    "    feature_matrix = np.zeros((num_lines, num_words))\n",
    "    \n",
    "    with open(infile, 'r') as fid:\n",
    "        ### ========== TODO : START ========== ###\n",
    "        # part 1b: process each line to populate feature_matrix\n",
    "        row = 0\n",
    "        for line in fid:\n",
    "            col = 0\n",
    "            new_words = extract_words(line)\n",
    "            for words in new_words:\n",
    "                if words in word_list:\n",
    "                    feature_matrix[row,word_list[words]] = 1\n",
    "                else:\n",
    "                    pass\n",
    "            row += 1\n",
    "        ### ========== TODO : END ========== ###\n",
    "        \n",
    "    return feature_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# functions -- evaluation\n",
    "######################################################################\n",
    "\n",
    "def performance(y_true, y_pred, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Calculates the performance metric based on the agreement between the \n",
    "    true labels and the predicted labels.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        y_true -- numpy array of shape (n,), known labels\n",
    "        y_pred -- numpy array of shape (n,), (continuous-valued) predictions\n",
    "        metric -- string, option used to select the performance measure\n",
    "                  options: 'accuracy', 'f1-score', 'auroc', 'precision',\n",
    "                           'sensitivity', 'specificity'        \n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        score  -- float, performance score\n",
    "    \"\"\"\n",
    "    # map continuous-valued predictions to binary labels\n",
    "    y_label = np.sign(y_pred)\n",
    "    y_label[y_label==0] = 1\n",
    "    \n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 2a: compute classifier performance\n",
    "    if metric == 'accuracy':\n",
    "        return metrics.accuracy_score(y_true, y_label)\n",
    "    if metric == 'f1_score':\n",
    "        return metrics.f1_score(y_true, y_label)\n",
    "    if metric == 'auroc':\n",
    "        return metrics.roc_auc_score(y_true, y_label)\n",
    "    if metric == 'precision':\n",
    "        return metrics.precision_score(y_true, y_label)\n",
    "    else:\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_label).ravel()\n",
    "        if metric == 'sensitivity':\n",
    "            sensitivity = tp/(tp + fn)\n",
    "            return sensitivity\n",
    "        if metric == 'specificity':\n",
    "            specificity = tn/(fp + tn)\n",
    "            return specificity\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def cv_performance(clf, X, y, kf, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Splits the data, X and y, into k-folds and runs k-fold cross-validation.\n",
    "    Trains classifier on k-1 folds and tests on the remaining fold.\n",
    "    Calculates the k-fold cross-validation performance metric for classifier\n",
    "    by averaging the performance across folds.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        clf    -- classifier (instance of SVC)\n",
    "        X      -- numpy array of shape (n,d), feature vectors\n",
    "                    n = number of examples\n",
    "                    d = number of features\n",
    "        y      -- numpy array of shape (n,), binary labels {1,-1}\n",
    "        kf     -- cross_validation.KFold or cross_validation.StratifiedKFold\n",
    "        metric -- string, option used to select performance measure\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        score   -- float, average cross-validation performance across k folds\n",
    "    \"\"\"\n",
    "    \n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 2b: compute average cross-validation performance    \n",
    "    mean = 0\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.decision_function(X_test)\n",
    "        mean += performance(y_test,y_pred,metric)\n",
    "    \n",
    "    return mean/5\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def select_param_linear(X, y, kf, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Sweeps different settings for the hyperparameter of a linear-kernel SVM,\n",
    "    calculating the k-fold CV performance for each setting, then selecting the\n",
    "    hyperparameter that 'maximize' the average k-fold CV performance.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        X      -- numpy array of shape (n,d), feature vectors\n",
    "                    n = number of examples\n",
    "                    d = number of features\n",
    "        y      -- numpy array of shape (n,), binary labels {1,-1}\n",
    "        kf     -- cross_validation.KFold or cross_validation.StratifiedKFold\n",
    "        metric -- string, option used to select performance measure\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        C -- float, optimal parameter value for linear-kernel SVM\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Linear SVM Hyperparameter Selection based on ', str(metric), ':')\n",
    "    max_score = 0\n",
    "    \n",
    "    C_range = 10.0 ** np.arange(-3, 3)\n",
    "    c = 0\n",
    "    for i in np.nditer(C_range):\n",
    "        clf = SVC(kernel='linear', C=i)\n",
    "        score = cv_performance(clf, X, y, kf, metric)\n",
    "        print('C = ', i, ' score = ', round(score, 4))\n",
    "        if score > max_score:\n",
    "            c = i\n",
    "            max_score = score\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 2c: select optimal hyperparameter using cross-validation\n",
    "    return c\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def select_param_rbf(X, y, kf, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Sweeps different settings for the hyperparameters of an RBF-kernel SVM,\n",
    "    calculating the k-fold CV performance for each setting, then selecting the\n",
    "    hyperparameters that 'maximize' the average k-fold CV performance.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        X       -- numpy array of shape (n,d), feature vectors\n",
    "                     n = number of examples\n",
    "                     d = number of features\n",
    "        y       -- numpy array of shape (n,), binary labels {1,-1}\n",
    "        kf     -- cross_validation.KFold or cross_validation.StratifiedKFold\n",
    "        metric  -- string, option used to select performance measure\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        gamma, C -- tuple of floats, optimal parameter values for an RBF-kernel SVM\n",
    "    \"\"\"\n",
    "    \n",
    "    print('RBF SVM Hyperparameter Selection based on ' + str(metric) + ':')\n",
    "    \n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 3b: create grid, then select optimal hyperparameters using cross-validation\n",
    "    C_range = 10.0 ** np.arange(-3, 3)\n",
    "    g_range = 10.0 ** np.arange(-3, 3)\n",
    "    c = 0\n",
    "    g = 0\n",
    "    for i in np.nditer(C_range):\n",
    "        for j in np.nditer(g_range):\n",
    "            clf = SVC(kernel='rbf', C=i, gamma=j)\n",
    "            score = cv_performance(clf, X, y, kf, metric)\n",
    "            print('C = ', i, ' score = ', round(score, 4))\n",
    "            if score > max_score:\n",
    "                c = i\n",
    "                g = j\n",
    "                max_score = score\n",
    "    return c,g\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def performance_test(clf, X, y, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Estimates the performance of the classifier using the 95% CI.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        clf          -- classifier (instance of SVC)\n",
    "                          [already fit to data]\n",
    "        X            -- numpy array of shape (n,d), feature vectors of test set\n",
    "                          n = number of examples\n",
    "                          d = number of features\n",
    "        y            -- numpy array of shape (n,), binary labels {1,-1} of test set\n",
    "        metric       -- string, option used to select performance measure\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        score        -- float, classifier performance\n",
    "    \"\"\"\n",
    "\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 4b: return performance on test data by first computing predictions and then calling performance\n",
    "\n",
    "    score = 0        \n",
    "    return score\n",
    "    ### ========== TODO : END ========== ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivrajgill/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:19: DeprecationWarning: 'U' mode is deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM Hyperparameter Selection based on  accuracy :\n",
      "C =  0.001  score =  0.7089\n",
      "C =  0.01  score =  0.7107\n",
      "C =  0.1  score =  0.806\n",
      "C =  1.0  score =  0.8146\n",
      "C =  10.0  score =  0.8182\n",
      "C =  100.0  score =  0.8182\n",
      "Linear SVM Hyperparameter Selection based on  f1_score :\n",
      "C =  0.001  score =  0.8297\n",
      "C =  0.01  score =  0.8306\n",
      "C =  0.1  score =  0.8755\n",
      "C =  1.0  score =  0.8749\n",
      "C =  10.0  score =  0.8766\n",
      "C =  100.0  score =  0.8766\n",
      "Linear SVM Hyperparameter Selection based on  auroc :\n",
      "C =  0.001  score =  0.5\n",
      "C =  0.01  score =  0.5031\n",
      "C =  0.1  score =  0.7188\n",
      "C =  1.0  score =  0.7531\n",
      "C =  10.0  score =  0.7592\n",
      "C =  100.0  score =  0.7592\n",
      "Linear SVM Hyperparameter Selection based on  precision :\n",
      "C =  0.001  score =  0.7089\n",
      "C =  0.01  score =  0.7102\n",
      "C =  0.1  score =  0.8357\n",
      "C =  1.0  score =  0.8562\n",
      "C =  10.0  score =  0.8595\n",
      "C =  100.0  score =  0.8595\n",
      "Linear SVM Hyperparameter Selection based on  sensitivity :\n",
      "C =  0.001  score =  1.0\n",
      "C =  0.01  score =  1.0\n",
      "C =  0.1  score =  0.9294\n",
      "C =  1.0  score =  0.9017\n",
      "C =  10.0  score =  0.9017\n",
      "C =  100.0  score =  0.9017\n",
      "Linear SVM Hyperparameter Selection based on  specificity :\n",
      "C =  0.001  score =  0.0\n",
      "C =  0.01  score =  0.0062\n",
      "C =  0.1  score =  0.5081\n",
      "C =  1.0  score =  0.6045\n",
      "C =  10.0  score =  0.6167\n",
      "C =  100.0  score =  0.6167\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "0.001\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "def main() :\n",
    "    np.random.seed(1234)\n",
    "    \n",
    "    # read the tweets and its labels   \n",
    "    dictionary = extract_dictionary('./tweets.txt')\n",
    "    X = extract_feature_vectors('./tweets.txt', dictionary)\n",
    "    y = read_vector_file('./labels.txt')\n",
    "    \n",
    "    metric_list = [\"accuracy\", \"f1_score\", \"auroc\", \"precision\", \"sensitivity\", \"specificity\"]\n",
    "    \n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 1c: split data into training (training + cross-validation) and testing set\n",
    "    train_features = X[:560,:]\n",
    "    train_labels = y[:560]\n",
    "    test_features = X[560:, :]\n",
    "    test_labels = y[560:]\n",
    "    c = [0]*6\n",
    "    i = 0\n",
    "    # part 2b: create stratified folds (5-fold CV)\n",
    "    kf = StratifiedKFold(n_splits=5)\n",
    "    # part 2d: for each metric, select optimal hyperparameter for linear-kernel SVM using CV\n",
    "    for metric in metric_list:\n",
    "        c[i] = select_param_linear(train_features, train_labels, kf, metric)\n",
    "        i += 1\n",
    "    # part 3c: for each metric, select optimal hyperparameter for RBF-SVM using CV\n",
    "    for j in range(len(c)):\n",
    "        print(c[j])\n",
    "    # part 4a: train linear- and RBF-kernel SVMs with selected hyperparameters\n",
    "    \n",
    "    # part 4c: report performance on test data\n",
    "    \n",
    "    ### ========== TODO : END ========== ###\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\" :\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivrajgill/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:19: DeprecationWarning: 'U' mode is deprecated\n"
     ]
    }
   ],
   "source": [
    "word_list = extract_dictionary('./tweets.txt')\n",
    "features = extract_feature_vectors('./tweets.txt', word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = features[:560, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = np.zeros([3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z[2,1] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-2cd6ee2c70b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'c' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
